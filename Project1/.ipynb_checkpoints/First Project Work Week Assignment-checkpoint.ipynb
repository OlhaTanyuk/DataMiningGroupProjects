{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Project Work Week Assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "_Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). Describe how you would define and measure the outcomes from the dataset. That is, why is this data important and how do you know if you have mined useful knowledge from the dataset? How would you measure the effectiveness of a good prediction algorithm? Be specific._\n",
    "\n",
    "__Goal__\n",
    "\n",
    "Predict default yes/no given the data.\n",
    "\n",
    " - Can we predict with X level of confidence?\n",
    " - Can I (something something banking jargon) interest rates (Paul will fill out)\n",
    " - Maximize sensitivity even if false positives.\n",
    " - Logistic Regression?\n",
    " - Sensitivity and Specificity (Need to determine what level is appropriate?)\n",
    "\n",
    "20% of the observations are default scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Meaning Type\n",
    "\n",
    "_Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file._\n",
    "\n",
    " - Saved the excel file as .csv for eaier input to python code.\n",
    " - .....yada yada\n",
    " - .....blah blah blah\n",
    "Data Set Information  \n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#\n",
    "\n",
    "__DD: Copied and pasted. We should reword this later__  \n",
    "\n",
    "_This research aimed at the case of customersâ€™ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel â€œSorting Smoothing Methodâ€ to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default._\n",
    "\n",
    "### Attribute Information\n",
    "__DD: Copied and pasted. We should reword this later__  \n",
    "\n",
    "_This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:_ \n",
    "\n",
    " - X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    " - X2: Gender (1 = male; 2 = female).\n",
    " - X3: Education (1=graduate school, 2=university, 3=high school, 4=others).\n",
    " - X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    " - X5: Age (year). \n",
    " - X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows:\n",
    "     -  X6 = the repayment status in September, 2005;\n",
    "     -  X7 = the repayment status in August, 2005; . . .;\n",
    "     -  X11 = the repayment status in April, 2005. The measurement scale for the repayment status is:\n",
    "         -  -1 = pay duly;\n",
    "         -  1 = payment delay for one month;\n",
    "         -  2 = payment delay for two months; . . .;\n",
    "         -  8 = payment delay for eight months;\n",
    "         -  9 = payment delay for nine months and above. \n",
    " - X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. \n",
    " - X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "\n",
    "_Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justifications for your methods._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Input/DefaultCreditcardClients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine datatypes\n",
    "print (df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the ordinal, float and categorical datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Change numeric values to floats\n",
    "continuous_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2','BILL_AMT3',\n",
    "                       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "                       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5',\n",
    "                       'PAY_AMT6']\n",
    "#ordinal_features = ['EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "#categ_features = ['ID', 'SEX', 'default payment next month'];\n",
    "df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "#df[ordinal_features] = df[ordinal_features].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Statistics\n",
    "\n",
    "_Visualize appropriate statistics (e.g., range, mode, mean, median, variance, counts) for a subset of attributes. Describe anything meaningful you found from this or if you found something potentially interesting. Note: You can also use data from other sources for comparison. Explain why the statistics run are meaningful. _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attributes\n",
    "\n",
    "_Visualize the most interesting attributes (at least 5 attributes, your opinion on what is interesting). Important: Interpret the implications for each visualization. Explain for each attribute why the chosen visualization is appropriate._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Joint Attributes\n",
    "\n",
    "_Visualize relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Attributes and Class\n",
    "\n",
    "_Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features\n",
    "\n",
    "_Are there other features that could be added to the data or created from existing features? Which ones?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "_You have free reign to provide additional analyses. One idea: implement dimensionality reduction, then visualize and interpret the results._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Formatting Cheatsheet\n",
    "https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "    \n",
    "__Copied from Syllabus__  \n",
    "You are to perform analysis of a data set: exploring the statistical summaries of the features,\n",
    "visualizing the attributes, and making conclusions from the visualizations and analysis. Follow the\n",
    "CRISP-DM framework in your analysis (you are not performing all of the CRISP-DM outline, only\n",
    "the portions relevant to understanding and visualization). This report is worth 20% of the final\n",
    "grade. Please upload a report (one per team) with all code used, visualizations, and text in a single\n",
    "document. The format of the document can be PDF, *.ipynb, or HTML. You can write the report in\n",
    "whatever format you like, but it is easiest to turn in the rendered Jupyter notebook.\n",
    "\n",
    "__A note on grading:__ This lab is mostly about visualizing and understanding your dataset. The\n",
    "largest share of the points is from how you interpret the visuals that you make. Making the visuals\n",
    "is not enough to satisfy each of the rubrics below—you should appropriately explain what the\n",
    "implications of the visualizations are. In other words, expect about 20% of the available points for\n",
    "visuals that have no substantive discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
